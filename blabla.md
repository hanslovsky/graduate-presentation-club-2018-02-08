# Title Slide
Good afternoon.  My name is Philipp Hanslovsky and I am currently a graduate fellow in the lab of Stephan Saalfeld.  As a computational lab, we focus on efficient and scalable image proccessing with application in the reconstruction of neural circuits from large light and electron microscopy data sets.  Today, I will present to you the metamorphisis of our in-lab proof-reading and annotation tool BigCAT.


# Circuit Reconstruction
What is neural circuit reconstruction?  In analogy to electric circuits, we want to create the wiring diagram for a nervous system.  In this analogy, neural fibres are the cables, and synapses are the wiring points.  The reconstruction and study of neural circuits is commonly referred to as *Connectomics*.  In order to create such a wiring diagram, we need to identify all individual neurons and their synapses.  To this day, the only imaging modality that can acquire data at the necessary resolution is electron microscopy.  For a sufficiently complex nervous system, e.g. drosophila brain with about 140000 neurons and three orders of magnitude more synapses, this is beyond what is feasible with human expert annotations only.  The use of machine learning techniques is thus imperative for the creation of the wiring diagram of a neural circuit.

# Automatic Reconstruction
Most current state of the art neuron reconstruction approaches follow a multi-step pipeline:  First, machine learning classifiers make boundary predictions for electron microscopy images (or raw data).  Currently, this is dominated by deep neural networks.  Based on these boundaries, voxels are grouped into so-called fragments via watershed transforms.  The fragments, in turn, are agglomerated into segments that, in the ideal case, represent individual neurons.  Typical agglomeration schemes are hierarchical clustering or multicuts.  The parameters for these models are usually trained with shallow machine learning algorithms, such as random forests.

Proof reading and user annotations play an important role in this reconstruction pipeline in many ways: Deep neural networks that are predominantly used for boundary predictions require massive amounts of training data from human expert annotations.  Incorrect fragments require corrections, and user annotations provide training data and impose hard constraints for fragment agglomeration.

One of our lab's biggest contribution to connectomics was the creation of the CREMI challenge, spearheaded by Jan Funke.  CREMI is unique among EM circuit reconstruction challenges in the amount of annotated data that it provides, and the labeling of synapses and annotations of pre- and post-synaptic partners.  CREMI provides six fully annotated 5 micron cube volumes of serial section TEM of the adult fly brain.  Annotations were provided, in large parts, by Ala Haddad who was a summer student in our lab.  The available amount of data allows for proper training of deep neural networks, which is reflected by the dominance of deep learning methods in the leaderboards.  When comparing the number of annotated pixels to other EM segmentation challenges, you will be able to appreciate the amount of work that went into CREMI.

It was during the preparation of the CREMI challenge that we noticed that none of the existing tools satisfied our needs for the creation of labels and annotations, and thus we decided to spend the time and effort to create our own tool, BigCAT.  The "Big" in BigCAT refers to BigDataViewer that forms the core of BigCAT and was created by Tobias Pietzsch. CAT is short for collaborative annotation tool as BigCAT was intended to be used in a collaborative environment.

BigCAT is a simple single window application that shows a cross-section of the data and labels at arbitrary orientations.  Navigation and commands are driven by mouse interactions in combinations with key combinations.  The cross-section can be translated or rotated and the field of view can be changed by changing the level of zoom.  Users can select and highlight fragments and assign to or split the selected fragment from a segment or use a paint brush to paint a new or existing label into the data.  This is essential for generating labeled data from scratch, or correcting erroneous fragments.  Besides simple painting, BigCAT offers more advanced tools like flood fill, both 2D and 3D, or projecting a 2D labeling orthogonal to the current cross-section. On top of that users can label synapses and annotate pre- and post-synaptic partners.

While BigCAT has proved its worth as an annotation tool in the context of the CREMI challenge, it does not offer all the functionality that we desire: orthogonal 2D views, 3D visualization, and multi-resolution painting.

When we were in the finishing phase of the CREMI challenge, Jeremy Maitin-Shepard released Neuroglancer, a JavaScript based viewer for volumetric data that has arbitrary slicing, orthogonal views, and 3D visualization built in.  Neuroglancer is a great piece of software and we considered dropping BigCAT and adapting Neuroglancer to our needs.  In the end, we decided against neuro glancer because it does not support painting or client side generation of models for 3D visualization for arbitrary datasets.

This table is a summary of the requiremens that we based our decision on.  Note that, in principle, neuroglancer is capable of client-side image processing, but JavaScript imposes memory limitations (2GB for chrome) and allows only for single-threaded applications and lacks multi-dimensional image processing libraries.  With BigCAT, in contrast, we have access to imglib2, a powerful image processing framework for Java that we are familiar with and help maintaining.  We are currently working on turning all the red cross marks in the BigCAT column into green check marks in BigCAT v2.  I will skip the less challenging requirements, i.e. cross-sections and orthogonal views, and go into more detail on my efforts on semi-automatic agglomeration, multi-resolution painting, and 3D model generation, in that order. Finally, I will show an example for client-side image processing.

As the name suggests, semi-automatic agglomeration has its application in the agglomeration phase of the reconstruction pipeline.  Instead of agglomerating the fragments fully automatically, user annotations are used as hard constraints and as training examples for a machine learning classifier that predicts the parameters of the underlying agglomeration model.  With every user annotation, the classifer and the model get more refined.  In this proof of concept, the 2D fragments were generated from the boundary predictions by Constantin Pape (these predictions are not up to date!).  Multicuts are used for agglomeration and a random forest predicts multi cut parameters (edge weights) based on simple pairwise image features of fragments, also provided by Constantin Pape.  Initially, the random forest has not been trained and we do not agglomerate any fragments.  The random forest needs at least one example per class, splits and merges.  As the user goes on to provide these required examples, the random forest will be trained and the agglomeration model can be applied. With just two annotations, this gives a pretty good agglomeration result already.  The user can now go on and provide more annotations in regions where the agglomeration failed.  Keep in mind that this is a very simple example or proof of concept, and in a more general real world use case, the classifier would certainly require more training examples for the model to produce meaningful results.

Agglomeration fails whenever there are undersegmentation errors in the fragments.  These can only be reconciled with modifications of the affected fragments, which brings me to the next feature: (multi-resolution) painting.  Painting has two applications in the reconstruction pipeline: creation of (large amounts) ground truth data, and correction of fragment undersegmentation.
Let me first show you, what painting currently looks like in BigCAT v2.  The most basic painting feature allows you to paint the currently selected label with a variable size paint brush, as I am demonstrating by giving this smiling face a hat, because neuron reconstruction is fun.  More advanced painting tools are extremely helpful for splitting undersegmentation in fragments: First, I identify a fragment that I want to split in half.  For the lack of better choices, I arbitrarily select a neuron.  Then, I identify the cross-section at which I would like to split and cover all of that neuron with a new id within that cross-section.  In order to not affect any other neurons, I restrict the painting to the neuron of interested.  Finally, I flood fill on one of the sides of the dividing cross-section, and, finally, end up with two neurons.

Currently, all painting operations work on the highest resolution that is available for the viewer.  This becomes problematic when we zoom out very far and try to paint a label with a large paint brush.  The vast number of voxels that would need to be modified would leave the UI unresponsive.  If we paint at a lower scale level as appropriate, the number of modified voxels could be kept to a minimum and the UI would remain responsive.  Let me now give you an intuition into our concept of multi-resolution painting.

EXPLAIN MULTI RES PAINTING

Now that we can agglomerate and paint, we certainly would like to see a 3D representation of the neurons, either just for admiration or because 3D visualization uncovers certain kinds of reconstruction errors more reliably than looking at 2D cross-sections.

SHOW VIDEO

We decided to use triangle meshes for 3D representation, which has been a standard in 3D graphics for many years.  These meshes can be generated with the marching cubes algorithm.  In a naive approach, the marching cubes would need to visit every voxel of a volume in order to generate a triangle mesh.  For obvious reasons, this is very inefficent and should be avoided at all cost.  Instead, we generally assume that individual neurons are sparse within a volume, and only look at the relevant voxels.  That means that when we request the meshes for a specific label id, we first need to identify all relevant locations.

EXPLAIN FIGURE

The client-side mesh generation concludes the presentation of core features of BigCAT v2 and I will now show an example of how we extended BigCAT v2 for a use case that it was not designed for initially.  We were interested in the distribution of synapses of projection neurons in the calyx. 

For that, we nee

This video shows the 3D visualization of a single neuron and all the synapses that are connected to it.

Constantin generated a segmentation for a larger sample of the FAFB dataset (sample E, dimensions etc), using Jan Funke's gunpowder framework for training and prediction of a deep neural network.  Larissa Heinrich used a similar network architecture to predict synapses in the same volume.  We wanted to combine both predictions and 3D visualize an individual neuron and all the synapses that are connected to it as a way to visualize the density of synapses of projection neurons in the calyx.  This is not a trivial task.  First, the synapse predictions need to be thresholded on the fly.  
